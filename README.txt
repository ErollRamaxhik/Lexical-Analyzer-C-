In computer science, lexical analysis, lexing, or tokenization is a 
process of converting a sequence of tokens. A program that performs lexical analysis can also be called a lexer, 
tokenizer, or scanner (though scanner can be mistaken as it is also the name of the first stage of a lexer so it is not use as frequently).